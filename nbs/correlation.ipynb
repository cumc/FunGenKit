{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mysterious-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def fill_missing_with_row_means(data):\n",
    "    # Calculate means of rows ignoring NaNs\n",
    "    row_means = np.nanmean(data, axis=1)\n",
    "    # Find indices where NaN values are\n",
    "    inds = np.where(np.isnan(data))\n",
    "    # Replace NaNs with the mean of the respective row\n",
    "    data[inds] = np.take(row_means, inds[0])\n",
    "    return data\n",
    "\n",
    "def np_pearson_cor(x, y, yv, yvss):\n",
    "    # Derivation see: https://cancerdatascience.org/blog/posts/pearson-correlation/\n",
    "    if yv is None or yvss is None:\n",
    "        yv = y - y.mean(axis=1, keepdims=True)\n",
    "        # yvss = (yv * yv).sum(axis=1)\n",
    "        yvss = np.einsum('ij,ij->i', yv, yv)  # Memory-efficient sum of squares\n",
    "    xv = x - x.mean(axis=1, keepdims=True)\n",
    "    xvss = np.einsum('ij,ij->i', xv, xv)  # Memory-efficient sum of squares\n",
    "    # Use einsum for memory-efficient matrix multiplication\n",
    "    # result = np.matmul(xv, yv.T) / np.sqrt(np.outer(xvss, yvss))\n",
    "    result = np.einsum('ij,kj->ik', xv, yv) / np.sqrt(xvss[:, np.newaxis] * yvss[np.newaxis, :])\n",
    "\n",
    "    # Limit the result to the range [-1, 1]\n",
    "    np.clip(result, -1.0, 1.0, out=result)\n",
    "    return result    \n",
    "\n",
    "def correlation_matrix_by_rows(data, chunk_size, num_processes=None, return_upper_triangle=True):\n",
    "    n_rows = data.shape[0]\n",
    "    correlation_matrix = np.zeros((n_rows, n_rows))\n",
    "\n",
    "    print(\"Performing mean imputation for missing values ...\")\n",
    "    data = fill_missing_with_row_means(data)\n",
    "\n",
    "    print(\"Precompute matrix quantities ...\")\n",
    "    yv = data - data.mean(axis=1, keepdims=True)\n",
    "    yvss = np.einsum('ij,ij->i', yv, yv)  # Memory-efficient sum of squares\n",
    "\n",
    "    if num_processes is None or num_processes <= 1:\n",
    "        # Non-parallel execution\n",
    "        for start_row in range(0, n_rows, chunk_size):\n",
    "            end_row = min(start_row + chunk_size, n_rows)\n",
    "            print(\"Working on rows\", start_row, \"to\", end_row, \"out of\", n_rows, \"rows\")\n",
    "            correlation_matrix[start_row:end_row, :] = np_pearson_cor(data[start_row:end_row], data, yv, yvss)\n",
    "    else:\n",
    "        # Parallel execution\n",
    "        row_ranges = [(start, min(start + chunk_size, n_rows)) for start in range(0, n_rows, chunk_size)]\n",
    "        with Pool(processes=num_processes) as pool:\n",
    "            for idx, (start, end) in enumerate(row_ranges):\n",
    "                correlation_matrix[start:end, :] = pool.apply(correlation_chunk, (start, end, data, yv, yvss))\n",
    "\n",
    "    # Mirror the upper triangle to the lower triangle\n",
    "    if not return_upper_triangle:\n",
    "        i_lower = np.tril_indices(n_rows, -1)\n",
    "        correlation_matrix[i_lower] = correlation_matrix.T[i_lower]\n",
    "    return correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "crude-biology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing mean imputation for missing values ...\n",
      "Precompute matrix quantities ...\n",
      "Working on rows 0 to 100\n",
      "Working on rows 100 to 200\n",
      "Working on rows 200 to 300\n",
      "Working on rows 300 to 400\n",
      "Working on rows 400 to 500\n",
      "Working on rows 500 to 600\n",
      "Working on rows 600 to 700\n",
      "Working on rows 700 to 800\n",
      "Working on rows 800 to 900\n",
      "Working on rows 900 to 1000\n",
      "0.15220069885253906\n",
      "0.01983809471130371\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def test_correlation_matrix_by_rows():\n",
    "    # compare between customized iplementation and np.corr\n",
    "    data_matrix = np.random.rand(1000, 200)  # Example large data matrix\n",
    "    chunk_size = 100  # Define chunk size\n",
    "    num_processes = 0  # Number of parallel processes\n",
    "    time_custom = time.time()\n",
    "    correlation_matrix_custom = correlation_matrix_by_rows(data_matrix, chunk_size, num_processes, False)\n",
    "    time_custom = time.time() - time_custom\n",
    "    print(time_custom)\n",
    "    time_np = time.time()\n",
    "    correlation_matrix_np = np.corrcoef(data_matrix, rowvar=True)\n",
    "    time_np = time.time() - time_np\n",
    "    print(time_np)\n",
    "    accuracy = np.allclose(correlation_matrix_custom, correlation_matrix_np)\n",
    "    assert accuracy == True\n",
    "test_correlation_matrix_by_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-newsletter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing mean imputation for missing values ...\n",
      "Precompute matrix quantities ...\n",
      "Working on rows 0 to 10000\n"
     ]
    }
   ],
   "source": [
    "def benchmark_correlation_matrix_by_rows():\n",
    "    # 17K samples, pair-wise LD of 100,000 variants \n",
    "    data_matrix = np.random.rand(100000, 17000)  # Example large data matrix\n",
    "    chunk_size = 10000  # Define chunk size\n",
    "    num_processes = 0  # Number of parallel processes\n",
    "    time_custom = time.time()\n",
    "    correlation_matrix_custom = correlation_matrix_by_rows(data_matrix, chunk_size, num_processes)\n",
    "    time_custom = time.time() - time_custom\n",
    "    print(time_custom)\n",
    "benchmark_correlation_matrix_by_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_np_correlation():\n",
    "    # 17K samples, pair-wise LD of 100,000 variants \n",
    "    data_matrix = np.random.rand(100000, 17000)  # Example large data matrix\n",
    "    time_np = time.time()\n",
    "    correlation_matrix_np = np.corrcoef(data_matrix, rowvar=True)\n",
    "    time_np = time.time() - time_np\n",
    "    print(time_np)\n",
    "benchmark_np_correlation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
