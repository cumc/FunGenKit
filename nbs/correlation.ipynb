{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exceptional-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def fill_missing_with_row_means(data):\n",
    "    # Calculate means of rows ignoring NaNs\n",
    "    row_means = np.nanmean(data, axis=1)\n",
    "    # Find indices where NaN values are\n",
    "    inds = np.where(np.isnan(data))\n",
    "    # Replace NaNs with the mean of the respective row\n",
    "    data[inds] = np.take(row_means, inds[0])\n",
    "    return data\n",
    "\n",
    "def np_pearson_cor(x, y, yv, yvss):\n",
    "    # Derivation see: https://cancerdatascience.org/blog/posts/pearson-correlation/\n",
    "    if yv is None or yvss is None:\n",
    "        yv = y - y.mean(axis=1, keepdims=True)\n",
    "        # yvss = (yv * yv).sum(axis=1)\n",
    "        yvss = np.einsum('ij,ij->i', yv, yv)  # Memory-efficient sum of squares\n",
    "    xv = x - x.mean(axis=1, keepdims=True)\n",
    "    xvss = np.einsum('ij,ij->i', xv, xv)  # Memory-efficient sum of squares\n",
    "    # Use einsum for memory-efficient matrix multiplication\n",
    "    # result = np.matmul(xv, yv.T) / np.sqrt(np.outer(xvss, yvss))\n",
    "    result = np.einsum('ij,kj->ik', xv, yv) / np.sqrt(xvss[:, np.newaxis] * yvss[np.newaxis, :])\n",
    "\n",
    "    # Limit the result to the range [-1, 1]\n",
    "    np.clip(result, -1.0, 1.0, out=result)\n",
    "    return result\n",
    "\n",
    "def correlation_chunk(start_row, end_row, data, yv, yvss):\n",
    "    return np_pearson_cor(data[start_row:end_row], data, yv, yvss)\n",
    "\n",
    "def correlation_matrix_by_rows(data, chunk_size, num_processes=None, return_upper_triangle=True):\n",
    "    n_rows = data.shape[0]\n",
    "    # here we create this matrix up-front.\n",
    "    # it is also possible to not create this but to keep writing data to disk as they are generated\n",
    "    correlation_matrix = np.zeros((n_rows, n_rows))\n",
    "\n",
    "    print(\"Performing mean imputation for missing values ...\")\n",
    "    data = fill_missing_with_row_means(data)\n",
    "\n",
    "    print(\"Precompute matrix quantities ...\")\n",
    "    yv = data - data.mean(axis=1, keepdims=True)\n",
    "    yvss = np.einsum('ij,ij->i', yv, yv)  # Memory-efficient sum of squares\n",
    "\n",
    "    if num_processes is None or num_processes <= 1:\n",
    "        # Non-parallel execution\n",
    "        for start_row in range(0, n_rows, chunk_size):\n",
    "            end_row = min(start_row + chunk_size, n_rows)\n",
    "            print(\"Working on rows\", start_row, \"to\", end_row, \"out of\", n_rows, \"rows\")\n",
    "            correlation_matrix[start_row:end_row, :] = correlation_chunk(start_row, end_row, data, yv, yvss)\n",
    "    else:\n",
    "        # Parallel execution\n",
    "        row_ranges = [(start, min(start + chunk_size, n_rows)) for start in range(0, n_rows, chunk_size)]\n",
    "        with Pool(processes=num_processes) as pool:\n",
    "            for idx, (start, end) in enumerate(row_ranges):\n",
    "                correlation_matrix[start:end, :] = pool.apply(correlation_chunk, (start, end, data, yv, yvss))\n",
    "\n",
    "    # Mirror the upper triangle to the lower triangle\n",
    "    if not return_upper_triangle:\n",
    "        i_lower = np.tril_indices(n_rows, -1)\n",
    "        correlation_matrix[i_lower] = correlation_matrix.T[i_lower]\n",
    "    return correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-vanilla",
   "metadata": {},
   "source": [
    "First, test if the customized function gives same result as np default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "packed-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing mean imputation for missing values ...\n",
      "Precompute matrix quantities ...\n",
      "Working on rows 0 to 200 out of 1000 rows\n",
      "Working on rows 200 to 400 out of 1000 rows\n",
      "Working on rows 400 to 600 out of 1000 rows\n",
      "Working on rows 600 to 800 out of 1000 rows\n",
      "Working on rows 800 to 1000 out of 1000 rows\n",
      "0.1574692726135254\n",
      "0.021502971649169922\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def test_correlation_matrix_by_rows():\n",
    "    # compare between customized iplementation and np.corr\n",
    "    data_matrix = np.random.rand(1000, 200)  # Example large data matrix\n",
    "    chunk_size = 200  # Define chunk size\n",
    "    num_processes = 0  # Number of parallel processes\n",
    "    time_custom = time.time()\n",
    "    correlation_matrix_custom = correlation_matrix_by_rows(data_matrix, chunk_size, num_processes, False)\n",
    "    time_custom = time.time() - time_custom\n",
    "    print(time_custom)\n",
    "    time_np = time.time()\n",
    "    correlation_matrix_np = np.corrcoef(data_matrix, rowvar=True)\n",
    "    time_np = time.time() - time_np\n",
    "    print(time_np)\n",
    "    accuracy = np.allclose(correlation_matrix_custom, correlation_matrix_np)\n",
    "    assert accuracy == True\n",
    "test_correlation_matrix_by_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-longitude",
   "metadata": {},
   "source": [
    "Benchmark the np default approach: 50K by 50K matrix is 18Gb in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "significant-small",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142.4240164756775\n"
     ]
    }
   ],
   "source": [
    "def benchmark_np_correlation():\n",
    "    # 17K samples, pair-wise LD of 50,000 variants\n",
    "    data_matrix = np.random.rand(50000, 17000)  # Example large data matrix\n",
    "    time_np = time.time()\n",
    "    correlation_matrix_np = np.corrcoef(data_matrix, rowvar=True)\n",
    "    time_np = time.time() - time_np\n",
    "    print(time_np)\n",
    "benchmark_np_correlation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-benchmark",
   "metadata": {},
   "source": [
    "Benchmark the customized approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-transition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing mean imputation for missing values ...\n",
      "Precompute matrix quantities ...\n",
      "Working on rows 0 to 1000 out of 50000 rows\n",
      "Working on rows 1000 to 2000 out of 50000 rows\n",
      "Working on rows 2000 to 3000 out of 50000 rows\n",
      "Working on rows 3000 to 4000 out of 50000 rows\n",
      "Working on rows 4000 to 5000 out of 50000 rows\n",
      "Working on rows 5000 to 6000 out of 50000 rows\n",
      "Working on rows 6000 to 7000 out of 50000 rows\n",
      "Working on rows 7000 to 8000 out of 50000 rows\n",
      "Working on rows 8000 to 9000 out of 50000 rows\n",
      "Working on rows 9000 to 10000 out of 50000 rows\n",
      "Working on rows 10000 to 11000 out of 50000 rows\n",
      "Working on rows 11000 to 12000 out of 50000 rows\n",
      "Working on rows 12000 to 13000 out of 50000 rows\n",
      "Working on rows 13000 to 14000 out of 50000 rows\n",
      "Working on rows 14000 to 15000 out of 50000 rows\n",
      "Working on rows 15000 to 16000 out of 50000 rows\n",
      "Working on rows 16000 to 17000 out of 50000 rows\n",
      "Working on rows 17000 to 18000 out of 50000 rows\n",
      "Working on rows 18000 to 19000 out of 50000 rows\n",
      "Working on rows 19000 to 20000 out of 50000 rows\n",
      "Working on rows 20000 to 21000 out of 50000 rows\n",
      "Working on rows 21000 to 22000 out of 50000 rows\n",
      "Working on rows 22000 to 23000 out of 50000 rows\n",
      "Working on rows 23000 to 24000 out of 50000 rows\n",
      "Working on rows 24000 to 25000 out of 50000 rows\n",
      "Working on rows 25000 to 26000 out of 50000 rows\n",
      "Working on rows 26000 to 27000 out of 50000 rows\n",
      "Working on rows 27000 to 28000 out of 50000 rows\n",
      "Working on rows 28000 to 29000 out of 50000 rows\n",
      "Working on rows 29000 to 30000 out of 50000 rows\n",
      "Working on rows 30000 to 31000 out of 50000 rows\n",
      "Working on rows 31000 to 32000 out of 50000 rows\n",
      "Working on rows 32000 to 33000 out of 50000 rows\n",
      "Working on rows 33000 to 34000 out of 50000 rows\n",
      "Working on rows 34000 to 35000 out of 50000 rows\n",
      "Working on rows 35000 to 36000 out of 50000 rows\n",
      "Working on rows 36000 to 37000 out of 50000 rows\n",
      "Working on rows 37000 to 38000 out of 50000 rows\n",
      "Working on rows 38000 to 39000 out of 50000 rows\n",
      "Working on rows 39000 to 40000 out of 50000 rows\n",
      "Working on rows 40000 to 41000 out of 50000 rows\n",
      "Working on rows 41000 to 42000 out of 50000 rows\n",
      "Working on rows 42000 to 43000 out of 50000 rows\n",
      "Working on rows 43000 to 44000 out of 50000 rows\n"
     ]
    }
   ],
   "source": [
    "def benchmark_correlation_matrix_by_rows():\n",
    "    # 17K samples, pair-wise LD of 50,000 variants \n",
    "    data_matrix = np.random.rand(50000, 17000)  # Example large data matrix\n",
    "    chunk_size = 1000  # Define chunk size\n",
    "    num_processes = 0  # Number of parallel processes\n",
    "    time_custom = time.time()\n",
    "    correlation_matrix_custom = correlation_matrix_by_rows(data_matrix, chunk_size, num_processes)\n",
    "    time_custom = time.time() - time_custom\n",
    "    print(time_custom)\n",
    "benchmark_correlation_matrix_by_rows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
