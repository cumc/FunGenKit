{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "experienced-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def fill_missing_with_row_means(data):\n",
    "    # Calculate means of rows ignoring NaNs\n",
    "    row_means = np.nanmean(data, axis=1)\n",
    "    # Find indices where NaN values are\n",
    "    inds = np.where(np.isnan(data))\n",
    "    # Replace NaNs with the mean of the respective row\n",
    "    data[inds] = np.take(row_means, inds[0])\n",
    "    return data\n",
    "\n",
    "def np_pearson_cor(x, y, yv, yvss):\n",
    "    # Derivation see: https://cancerdatascience.org/blog/posts/pearson-correlation/\n",
    "    if yv is None or yvss is None:\n",
    "        yv = y - y.mean(axis=1, keepdims=True)\n",
    "        # yvss = (yv * yv).sum(axis=1)\n",
    "        yvss = np.einsum('ij,ij->i', yv, yv)  # Memory-efficient sum of squares\n",
    "    xv = x - x.mean(axis=1, keepdims=True)\n",
    "    xvss = np.einsum('ij,ij->i', xv, xv)  # Memory-efficient sum of squares\n",
    "    # Use einsum for memory-efficient matrix multiplication\n",
    "    # result = np.matmul(xv, yv.T) / np.sqrt(np.outer(xvss, yvss))\n",
    "    result = np.einsum('ij,kj->ik', xv, yv) / np.sqrt(xvss[:, np.newaxis] * yvss[np.newaxis, :])\n",
    "\n",
    "    # Limit the result to the range [-1, 1]\n",
    "    np.clip(result, -1.0, 1.0, out=result)\n",
    "    return result\n",
    "\n",
    "def correlation_chunk(start_row, end_row, data, yv, yvss):\n",
    "    return np_pearson_cor(data[start_row:end_row], data, yv, yvss)\n",
    "\n",
    "def correlation_matrix_by_rows(data, chunk_size, num_processes=None, return_upper_triangle=True):\n",
    "    n_rows = data.shape[0]\n",
    "    # here we create this matrix up-front.\n",
    "    # it is also possible to not create this but to keep writing data to disk as they are generated\n",
    "    correlation_matrix = np.zeros((n_rows, n_rows))\n",
    "\n",
    "    print(\"Performing mean imputation for missing values ...\")\n",
    "    data = fill_missing_with_row_means(data)\n",
    "\n",
    "    print(\"Precompute matrix quantities ...\")\n",
    "    yv = data - data.mean(axis=1, keepdims=True)\n",
    "    yvss = np.einsum('ij,ij->i', yv, yv)  # Memory-efficient sum of squares\n",
    "\n",
    "    if num_processes is None or num_processes <= 1:\n",
    "        # Non-parallel execution\n",
    "        for start_row in range(0, n_rows, chunk_size):\n",
    "            end_row = min(start_row + chunk_size, n_rows)\n",
    "            print(\"Working on rows\", start_row, \"to\", end_row, \"out of\", n_rows, \"rows\")\n",
    "            correlation_matrix[start_row:end_row, :] = correlation_chunk(start_row, end_row, data, yv, yvss)\n",
    "    else:\n",
    "        # Parallel execution\n",
    "        row_ranges = [(start, min(start + chunk_size, n_rows)) for start in range(0, n_rows, chunk_size)]\n",
    "        with Pool(processes=num_processes) as pool:\n",
    "            for idx, (start, end) in enumerate(row_ranges):\n",
    "                correlation_matrix[start:end, :] = pool.apply(correlation_chunk, (start, end, data, yv, yvss))\n",
    "\n",
    "    # Mirror the upper triangle to the lower triangle\n",
    "    if not return_upper_triangle:\n",
    "        i_lower = np.tril_indices(n_rows, -1)\n",
    "        correlation_matrix[i_lower] = correlation_matrix.T[i_lower]\n",
    "    return correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-speaking",
   "metadata": {},
   "source": [
    "First, test if the customized function gives same result as np default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "terminal-northern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing mean imputation for missing values ...\n",
      "Precompute matrix quantities ...\n",
      "Working on rows 0 to 200 out of 1000 rows\n",
      "Working on rows 200 to 400 out of 1000 rows\n",
      "Working on rows 400 to 600 out of 1000 rows\n",
      "Working on rows 600 to 800 out of 1000 rows\n",
      "Working on rows 800 to 1000 out of 1000 rows\n",
      "0.1574692726135254\n",
      "0.021502971649169922\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def test_correlation_matrix_by_rows():\n",
    "    # compare between customized iplementation and np.corr\n",
    "    data_matrix = np.random.rand(1000, 200)  # Example large data matrix\n",
    "    chunk_size = 200  # Define chunk size\n",
    "    num_processes = 0  # Number of parallel processes\n",
    "    time_custom = time.time()\n",
    "    correlation_matrix_custom = correlation_matrix_by_rows(data_matrix, chunk_size, num_processes, False)\n",
    "    time_custom = time.time() - time_custom\n",
    "    print(time_custom)\n",
    "    time_np = time.time()\n",
    "    correlation_matrix_np = np.corrcoef(data_matrix, rowvar=True)\n",
    "    time_np = time.time() - time_np\n",
    "    print(time_np)\n",
    "    accuracy = np.allclose(correlation_matrix_custom, correlation_matrix_np)\n",
    "    assert accuracy == True\n",
    "test_correlation_matrix_by_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-training",
   "metadata": {},
   "source": [
    "Benchmark the np default approach: 50K by 50K matrix is 18Gb in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "intermediate-xerox",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142.4240164756775\n"
     ]
    }
   ],
   "source": [
    "def benchmark_np_correlation():\n",
    "    # 17K samples, pair-wise LD of 50,000 variants\n",
    "    data_matrix = np.random.rand(50000, 17000)  # Example large data matrix\n",
    "    time_np = time.time()\n",
    "    correlation_matrix_np = np.corrcoef(data_matrix, rowvar=True)\n",
    "    time_np = time.time() - time_np\n",
    "    print(time_np)\n",
    "benchmark_np_correlation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-registrar",
   "metadata": {},
   "source": [
    "Benchmark the customized approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-personal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing mean imputation for missing values ...\n",
      "Precompute matrix quantities ...\n",
      "Working on rows 0 to 1000 out of 50000 rows\n"
     ]
    }
   ],
   "source": [
    "def benchmark_correlation_matrix_by_rows():\n",
    "    # 17K samples, pair-wise LD of 50,000 variants \n",
    "    data_matrix = np.random.rand(50000, 17000)  # Example large data matrix\n",
    "    chunk_size = 1000  # Define chunk size\n",
    "    num_processes = 0  # Number of parallel processes\n",
    "    time_custom = time.time()\n",
    "    correlation_matrix_custom = correlation_matrix_by_rows(data_matrix, chunk_size, num_processes)\n",
    "    time_custom = time.time() - time_custom\n",
    "    print(time_custom)\n",
    "benchmark_correlation_matrix_by_rows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
